{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from dotenv import load_dotenv\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "env_path = Path.cwd().parent / \".env\"\n",
        "print(\"env_path =\", env_path, \"exists =\", env_path.exists())\n",
        "\n",
        "load_dotenv(env_path)\n",
        "\n",
        "key = os.getenv(\"GEMINI_API_KEY\")\n",
        "print(\"GEMINI_API_KEY is None?\", key is None)\n",
        "if key:\n",
        "    print(\"key preview:\", key[:6] + \"...\" + key[-4:])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "WH1HUoijPK0E"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import requests\n",
        "import os\n",
        "from google import genai\n",
        "\n",
        "GEMINI_API_KEY = os.getenv('GEMINI_API_KEY')\n",
        "client = genai.Client(api_key = GEMINI_API_KEY)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google import genai\n",
        "\n",
        "client = genai.Client()\n",
        "resp = client.models.generate_content(\n",
        "    model=\"gemini-2.5-flash\",\n",
        "    contents=\"Hello\"\n",
        ")\n",
        "print(resp.text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "abqMFAvkXtfn"
      },
      "source": [
        "Set up Gemini client"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "id": "kaKBRrpHYyXf",
        "outputId": "bf9bcc72-27cb-4520-e8b5-c910fbb945af"
      },
      "outputs": [],
      "source": [
        "def send_request(prompt, max_tokens):\n",
        "    try:\n",
        "        response = client.models.generate_content(\n",
        "            model='gemini-2.5-flash-lite', # or gemini-2.5-flash\n",
        "            contents=[prompt],\n",
        "            config={\n",
        "                'max_output_tokens': max_tokens,\n",
        "                'temperature': 1.0,\n",
        "                'thinking_config': {\n",
        "                    'include_thoughts': True, # Set to True if you want to see the reasoning\n",
        "                    'thinking_budget': 0   # Minimum budget is 0 if we don't use a thinking model\n",
        "                }\n",
        "            },\n",
        "        )\n",
        "        if response:\n",
        "            print(f\"Prompt tokens: {response.usage_metadata.prompt_token_count}\")\n",
        "            print(f\"Thinking tokens: {response.usage_metadata.thoughts_token_count}\") # <--- This one\n",
        "            print(f\"Output tokens (Answer): {response.usage_metadata.candidates_token_count}\")\n",
        "            print(f\"Total tokens used: {response.usage_metadata.total_token_count}\")\n",
        "            return response.text\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "    return ''\n",
        "\n",
        "send_request(prompt=\"How are you?\", max_tokens=40)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "myenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
