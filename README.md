# CPSC-532V-LLM-based-Evaluation

## Overview

This implements an extract-then-summarize pipeline for long-document summarization, inspired by the Extract-then-Evaluate framework proposed by Wu et al. (2024). Long document summarization, generated by LLMs remains challenging due to high computational costs and the Lost-in-the-Middle problem, where salient information in the middle of long inputs is often under-attended.

Wu et al. (2024) address these challenges in the context of summary evaluation by first extracting a subset of salient sentences from the source document and then applying LLM-based evaluation on the condensed input. Their empirical results show that this strategy not only substantially reduces computational cost but also improves correlation with human judgments. In addition, the authors provide practical guidelines on extraction methods and effective document lengths.

Motivated by these findings, the present implementation adapts the core idea of ROUGE-guided sentence extraction to a summarization pipeline. Specifically, the system first generates a draft summary of the full document, then selects a subset of source sentences that maximally align with this draft using ROUGE recall, and finally produces a high-quality summary conditioned only on the extracted sentences. By doing so, the pipeline aims to preserve salient content while reducing input length, thereby improving robustness and faithfulness in long-document summarization.

---

## System Architecture

The pipeline consists of three passes.

### Pass 1: Draft Summarization

A draft summary is generated directly from the full document. The draft is not intended to be the final output; it serves as a guidance signal to identify which parts of the source text should be retained for the final summarization.

**Rationale:**  
Using a model-generated draft provides a compact “target” that captures salient themes. The extraction stage then aligns sentence selection with this target.

---

### Pass 2: ROUGE-based Sentence Extraction

The document is split into sentences using NLTK’s Punkt tokenizer. Each sentence is scored against the draft summary using ROUGE recall. The pipeline supports:

- ROUGE-1 recall  
- ROUGE-2 recall  
- ROUGE-1 + ROUGE-2 recall (combined score)

Sentences are then sorted by score and greedily selected until an extraction budget is reached. Finally, selected sentences are re-ordered by their original position to preserve readability.

---

### Pass 3: Final Summarization from Extracted Evidence

A final summary is produced using only the extracted sentences. The prompt explicitly instructs the model to remain faithful and not introduce unsupported facts.

**Rationale:**  
By summarizing a condensed, high-salience subset, the model focuses on relevant content and reduces sensitivity to irrelevant or redundant sections in the original document.
